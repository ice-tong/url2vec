{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191876659\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request as request\n",
    "\n",
    "# Step 1: Download the data.\n",
    "url = 'http://research.larc.smu.edu.sg/urldetection/static/samples/'\n",
    "\n",
    "\n",
    "def maybe_download(filename):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = request.urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  print(statinfo.st_size)\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('urlsample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong URL:  &amp;.rand=13vqcr8bp0gud&amp;lc=1033&amp;id=64855&amp;mkt=en-us&amp;cbcxt=mai&amp;snsc=1\"\n",
      "\n",
      "Wrong URL:  &amp;.rand=13vqcr8bp0gud&amp;lc=1033&amp;id=64855&amp;mkt=en-us&amp;cbcxt=mai&amp;snsc=1\"\n",
      "\n",
      "Data size 2558816\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    urls = []\n",
    "    with open(filename) as f:\n",
    "       lines = f.readlines()\n",
    "       for line in lines:\n",
    "           try:\n",
    "               test_url = re.split(r'\\t+', line)[1][:-1]\n",
    "               urls.append(test_url)\n",
    "           except IndexError:\n",
    "               print(\"Wrong URL: \", line)\n",
    "               continue\n",
    "    return urls\n",
    "\n",
    "\n",
    "urls = read_data(filename)\n",
    "print('Data size', len(urls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_chars = []\n",
    "for url in urls: \n",
    "    chars = list(url)\n",
    "    url_chars.extend(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len by char:  181641215\n",
      "Len by url:  2558816\n"
     ]
    }
   ],
   "source": [
    "print(\"Len by char: \", len(url_chars))\n",
    "print(\"Len by url: \", len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 0], ('t', 15694498), ('/', 12949094), ('e', 8642312), ('s', 8244125)]\n",
      "Sample data [33, 1, 12, 28, 2, 2, 17, 21, 21, 13] ['f', 't', 'p', ':', '/', '/', '1', '8', '8', '.']\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# Step 2: Build the dictionary and replace rare words with UNK token.\n",
    "vocabulary_size = 500\n",
    "\n",
    "def build_dataset(words, vocabulary_size):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count += 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(url_chars, vocabulary_size)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of unique chars:  122\n"
     ]
    }
   ],
   "source": [
    "print(\"no. of unique chars: \", len(reverse_dictionary))\n",
    "vocabulary_size = len(reverse_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 t -> 33 f\n",
      "1 t -> 12 p\n",
      "12 p -> 1 t\n",
      "12 p -> 28 :\n",
      "28 : -> 2 /\n",
      "28 : -> 12 p\n",
      "2 / -> 2 /\n",
      "2 / -> 28 :\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Step 3: Function to generate a training batch for the skip-gram model.\n",
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [skip_window]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "  data_index = (data_index + len(data) - span) % len(data)\n",
    "  return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "  print(batch[i], reverse_dictionary[batch[i]],\n",
    "        '->', labels[i, 0], reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 4: Build and train a skip-gram model.\n",
    "import math\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "  with tf.device('/cpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Compute the average NCE loss for the batch.\n",
    "  # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "  # time we evaluate the loss.\n",
    "  loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=train_labels,\n",
    "                     inputs=embed,\n",
    "                     num_sampled=num_sampled,\n",
    "                     num_classes=vocabulary_size))\n",
    "\n",
    "  # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "      normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "  # Add variable initializer.\n",
    "  init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  66.2353973389\n",
      "Nearest to P: $, \u0014, \u0017, Œ, i, O, |, T,\n",
      "Nearest to p: ´,  , f, †, 2, I, q, ],\n",
      "Nearest to 0: ,, L, z, ½, Y, @, ., (,\n",
      "Nearest to !: ¯, h, *, °, Ã, â, ¢, j,\n",
      "Nearest to ¯: ½, !, x, z, Ã, ., L, k,\n",
      "Nearest to B: S,  , \", !, =, -, ©, \u0016,\n",
      "Nearest to Z: 8, Æ, M, X, ‚, R, ¿, g,\n",
      "Nearest to V: ~, R, â, \u0015, \u0014, o, %, ',\n",
      "Nearest to \": â, B, ], `, l, z, n, ´,\n",
      "Nearest to r: 7, q, A,  , }, ï, ~, \\,\n",
      "Nearest to @: °, U, 3, ½, €, 8, 0, S,\n",
      "Nearest to S: B, ¡, &, ', 3, \\, @, #,\n",
      "Nearest to U: @, \\, ¡, 8, \u0017, ï, Z, b,\n",
      "Nearest to K: †, ’, ., \u0013, M, #, ¯, x,\n",
      "Nearest to Q: T, ), \u0014, \\, N, a, C, Ð,\n",
      "Nearest to 4: g, ,, ¿, `, š, t, |, †,\n",
      "Average loss at step  2000 :  3.97783042109\n",
      "Average loss at step  4000 :  3.79057035887\n",
      "Average loss at step  6000 :  3.85294621241\n",
      "Average loss at step  8000 :  3.92416106558\n",
      "Average loss at step  10000 :  3.88318236607\n",
      "Nearest to P: $, i, r, \u0017, f, 3, ¿, t,\n",
      "Nearest to p: t, /, \\, a, s, ‚, m, n,\n",
      "Nearest to 0: 3, *, ‚, 5, a, d, 8, 4,\n",
      "Nearest to !: *, ¯, j, Ã, B, D, ƒ, â,\n",
      "Nearest to ¯: ½, x, z, !, Ã, ., *, s,\n",
      "Nearest to B: S,  , ‚, :, \", !, -, y,\n",
      "Nearest to Z: 8, ‚, M, U, X, ¿, :, Æ,\n",
      "Nearest to V: R, 6, ~, â, H, o, A, l,\n",
      "Nearest to \": p, ., l, \\, â, a, B, n,\n",
      "Nearest to r: n, ‚, l, m, /, o, ., \\,\n",
      "Nearest to @: 3, d, °, U, ., 0, i, g,\n",
      "Nearest to S: B, s, 3, n, \\, ', &, @,\n",
      "Nearest to U: @, Z, \\, b, _, \u0017, F, 3,\n",
      "Nearest to K: M, b, †, :, 9, #, 4, D,\n",
      "Nearest to Q: T, ), ‚, \\, C, N, $, Â,\n",
      "Nearest to 4: g, ,, ‚, ¿, 1, 6, 0, e,\n",
      "Average loss at step  12000 :  3.82844892979\n",
      "Average loss at step  14000 :  3.87437379217\n",
      "Average loss at step  16000 :  3.92788596547\n",
      "Average loss at step  18000 :  3.96178900921\n",
      "Average loss at step  20000 :  3.88558880019\n",
      "Nearest to P: $, f, \u0017, r, t, ¿, 3, T,\n",
      "Nearest to p: t, e, o, m, ‚, /, i, \\,\n",
      "Nearest to 0: 3, 2, 6, 4, 1, 5, 7, ‚,\n",
      "Nearest to !: ¯, *, j, Ã, B, D, ¢, â,\n",
      "Nearest to ¯: ½, x, !, Ã, z, ., 1, l,\n",
      "Nearest to B: S,  , ‚, -, y, :, !, \",\n",
      "Nearest to Z: 8, M, ‚, U, X, A, ¿, Æ,\n",
      "Nearest to V: R, s, â, ~, 6, y, H, A,\n",
      "Nearest to \": p, l, \\, â, m, a, B, ],\n",
      "Nearest to r: m, n, ‚, l, s, i, t, h,\n",
      "Nearest to @: ., °, d, U, 3, g, r, _,\n",
      "Nearest to S: B, s, 3, ', 1, l, O, ¡,\n",
      "Nearest to U: Z, @, \\, 8, _, d, j, h,\n",
      "Nearest to K: M, c, 4, 9, #, 2, †, 3,\n",
      "Nearest to Q: T, ), ‚, c, \\, C, N, 4,\n",
      "Nearest to 4: 0, 1, 6, 2, ‚, 3, 7, ,,\n",
      "Average loss at step  22000 :  3.8118618089\n",
      "Average loss at step  24000 :  3.95478325617\n",
      "Average loss at step  26000 :  3.79159208179\n",
      "Average loss at step  28000 :  3.86993496728\n",
      "Average loss at step  30000 :  3.89164792514\n",
      "Nearest to P: r, $, f, \u0014, \u0017, t, ¿, M,\n",
      "Nearest to p: ‚, t, c, /, \\, o, e, m,\n",
      "Nearest to 0: 3, 2, 9, 6, 4, 5, 8, ‚,\n",
      "Nearest to !: ¯, *, j, Ã, B, D, ¢, â,\n",
      "Nearest to ¯: ½, x, !, 1, Ã, l, z, .,\n",
      "Nearest to B: S,  , -, !, :, ‚, g, y,\n",
      "Nearest to Z: ‚, U, M, X, ., :, Æ, Y,\n",
      "Nearest to V: R, â, ~, 9, \u0014, J, H, y,\n",
      "Nearest to \": m, \\, â, l, p, b, B, e,\n",
      "Nearest to r: n, m, ‚, s, l, w, h, t,\n",
      "Nearest to @: °, ., U, s, d, t, f, €,\n",
      "Nearest to S: B, l, ', O, 3, /, g, :,\n",
      "Nearest to U: Z, @, \\, _, F, j, \u0017, ¡,\n",
      "Nearest to K: 2, 3, M, 9, D, †, #, Ã,\n",
      "Nearest to Q: T, C, a, ), \u0014, 4, ‚, N,\n",
      "Nearest to 4: 1, 6, 7, 2, 5, 3, 9, ‚,\n",
      "Average loss at step  32000 :  3.85206820142\n",
      "Average loss at step  34000 :  3.85650522745\n",
      "Average loss at step  36000 :  3.88090783072\n",
      "Average loss at step  38000 :  3.7480757947\n",
      "Average loss at step  40000 :  3.70111948442\n",
      "Nearest to P: $, f, \u0017, r, M, \u0014, u, ¿,\n",
      "Nearest to p: s, m, l, /, c, \\, t, e,\n",
      "Nearest to 0: 3, 1, 2, 6, 9, 7, 5, 4,\n",
      "Nearest to !: ¯, B, Ã, *, â, ¢, H, ƒ,\n",
      "Nearest to ¯: x, ½, !, Ã, z, ., k, L,\n",
      "Nearest to B: S,  , !, b, \", -, :, =,\n",
      "Nearest to Z: X, ‚, U, M, Æ, A, 8, 4,\n",
      "Nearest to V: R, 9, â, ~, J, A, k, \u0014,\n",
      "Nearest to \": p, \\, â, m, 1, l, 0, /,\n",
      "Nearest to r: n, s, ‚, m, l, d, t, g,\n",
      "Nearest to @: °, ., d, U, _, r, t, s,\n",
      "Nearest to S: B, 6, 3, l, a, :, b, O,\n",
      "Nearest to U: Z, @, 0, 2, \\, j, \u0017, F,\n",
      "Nearest to K: 2, M, 8, 3, X, #, 9, :,\n",
      "Nearest to Q: T, 4, C, \u0014, ), N, \\, ‚,\n",
      "Nearest to 4: 1, 9, 0, 3, 7, 5, 8, 6,\n",
      "Average loss at step  42000 :  3.90042506301\n",
      "Average loss at step  44000 :  3.59393334281\n",
      "Average loss at step  46000 :  3.80834328759\n",
      "Average loss at step  48000 :  3.92132277572\n",
      "Average loss at step  50000 :  3.91742866659\n",
      "Nearest to P: $, \u0014, M, \u0017, T, 7, g, f,\n",
      "Nearest to p: t, m, s, \\, h, /, c, i,\n",
      "Nearest to 0: 7, 3, 8, 9, 2, 6, 1, 4,\n",
      "Nearest to !: ¯, 7, B, Ã, *, â, ¢, H,\n",
      "Nearest to ¯: ½, x, !, Ã, z, n, h, 1,\n",
      "Nearest to B: S, -,  , !, e, m, ‚, 7,\n",
      "Nearest to Z: 8, X, U, A, 4, ‚, Y, 9,\n",
      "Nearest to V: 9, R, k, 3, J, â, ~, d,\n",
      "Nearest to \": p, \\, â, ], 1, B, l, m,\n",
      "Nearest to r: n, ‚, l, ., -, t, h, s,\n",
      "Nearest to @: °, ., U, d, r, _, y, €,\n",
      "Nearest to S: B, 3, 7, ', 2, :, 6, 4,\n",
      "Nearest to U: Z, j, @, 8, 0, 6, :, 2,\n",
      "Nearest to K: 8, M, 4, 9, :, b, 6, X,\n",
      "Nearest to Q: T, 0, C, 2, 4, 9, N, \u0014,\n",
      "Nearest to 4: 7, 5, 9, 0, 8, 3, 1, 6,\n",
      "Average loss at step  52000 :  3.87485259211\n",
      "Average loss at step  54000 :  3.90785560894\n",
      "Average loss at step  56000 :  3.93176713705\n",
      "Average loss at step  58000 :  3.95141538453\n",
      "Average loss at step  60000 :  3.89167348313\n",
      "Nearest to P: m, f, M, $, ¿, \u0014, g, \u0017,\n",
      "Nearest to p: m, t, l, e, i, c, s, \\,\n",
      "Nearest to 0: 8, 5, 6, 3, 1, 4, 7, 9,\n",
      "Nearest to !: ¯, B, *, Ã, j, â, °, ¢,\n",
      "Nearest to ¯: ½, !, Ã, x, o, 1, n, z,\n",
      "Nearest to B: m, S,  , !, -, 4, \", ¢,\n",
      "Nearest to Z: X, A, U, ‚, Y, Æ, M, .,\n",
      "Nearest to V: R, 9, J, k, â, ~, A, \u0014,\n",
      "Nearest to \": l, â, /, \\, 1, m, h, ],\n",
      "Nearest to r: n, m, s, ‚, l, b, d, y,\n",
      "Nearest to @: °, k, d, ., U, i, y, _,\n",
      "Nearest to S: s, l, m, B, r, :, O, z,\n",
      "Nearest to U: Z, j, :, @, 0, _, i, A,\n",
      "Nearest to K: M, 8, X, 6, :, ’, b, #,\n",
      "Nearest to Q: T, C, 4, ‚, \u0014, -, /, \\,\n",
      "Nearest to 4: 5, 7, 8, 9, 1, 6, 0, 3,\n",
      "Average loss at step  62000 :  3.94890486836\n",
      "Average loss at step  64000 :  3.867705832\n",
      "Average loss at step  66000 :  3.84017916071\n",
      "Average loss at step  68000 :  3.88842541265\n",
      "Average loss at step  70000 :  3.9848043009\n",
      "Nearest to P: M, m, k, $, w, T, 7, v,\n",
      "Nearest to p: t, \\, l, m, n, u, ‚, /,\n",
      "Nearest to 0: 5, 8, 6, 3, 9, 7, 4, 2,\n",
      "Nearest to !: ¯, B, *, Ã, ¢, â, j, °,\n",
      "Nearest to ¯: ½, !, Ã, w, x, o, 1, z,\n",
      "Nearest to B: 5, -, S, 0, !,  , m, ‚,\n",
      "Nearest to Z: U, -, X, Y, ‚, 5, A, Æ,\n",
      "Nearest to V: R, J, 9, 6, ~, 3, \u0014, F,\n",
      "Nearest to \": p, l, t, â, m, \\, h, =,\n",
      "Nearest to r: s, n, m, ‚, l, k, -, t,\n",
      "Nearest to @: r, °, ., k, 6, €, U, †,\n",
      "Nearest to S: 0, v, B, 6, 2, ', 3, 5,\n",
      "Nearest to U: Z, j, :, 6, y, 0, F, @,\n",
      "Nearest to K: b, M, X, D, ’, 3, 6, 8,\n",
      "Nearest to Q: T, a, C, N, 9, \u0014, ‚, \\,\n",
      "Nearest to 4: 5, 9, 7, 8, 6, 1, 3, 0,\n",
      "Average loss at step  72000 :  3.73565837085\n",
      "Average loss at step  74000 :  3.77867855716\n",
      "Average loss at step  76000 :  3.76553732955\n",
      "Average loss at step  78000 :  3.87959844172\n",
      "Average loss at step  80000 :  3.78793599665\n",
      "Nearest to P: M, T, $, r, \u0017, \u0014, A, ¿,\n",
      "Nearest to p: m, t, l, s, ?, \\, r, w,\n",
      "Nearest to 0: 5, 9, 7, 6, 3, 8, 4, 2,\n",
      "Nearest to !: ¯, B, *, Ã, °, ¢, â, ƒ,\n",
      "Nearest to ¯: ½, o, !, Ã, 1, x, w, n,\n",
      "Nearest to B: -, 3, S, 5, !, 0, Y,  ,\n",
      "Nearest to Z: M, X, Y, U, R, -, Æ, N,\n",
      "Nearest to V: R, J, 9, 5, ~, 6, 0, B,\n",
      "Nearest to \": m, p, ?, t, â, \\, B, =,\n",
      "Nearest to r: n, s, m, l, p, g, ‚, u,\n",
      "Nearest to @: l, k, °, /, ., €, _, g,\n",
      "Nearest to S: s, B, t, ', l, A, \\, 0,\n",
      "Nearest to U: :, j, Z, -, _, F, W, @,\n",
      "Nearest to K: 0, X, 8, M, D, 5, 9, ’,\n",
      "Nearest to Q: T, N, C, 9, \u0014, ), ‚, \\,\n",
      "Nearest to 4: 5, 3, 7, 6, 8, 0, 9, 1,\n",
      "Average loss at step  82000 :  3.83685072076\n",
      "Average loss at step  84000 :  3.79714809608\n",
      "Average loss at step  86000 :  3.77027718323\n",
      "Average loss at step  88000 :  3.75433125901\n",
      "Average loss at step  90000 :  3.77773632562\n",
      "Nearest to P: M, T, v, $, \u0014, A, Œ, D,\n",
      "Nearest to p: m, t, e, s, l, c, /, \\,\n",
      "Nearest to 0: 8, 3, 6, 9, 5, 7, 4, 1,\n",
      "Nearest to !: ¯, B, *, °, ¢, Ã, â, ƒ,\n",
      "Nearest to ¯: ½, !, Ã, x, z, o, r, 1,\n",
      "Nearest to B: S, Y, !,  , O, -, M, ‚,\n",
      "Nearest to Z: Y, U, X, M, -, 5, Æ, R,\n",
      "Nearest to V: J, R, 5, 9, ~, ', N, k,\n",
      "Nearest to \": p, m, =, â, \\, ?, t, ],\n",
      "Nearest to r: l, n, m, s, ‚, u, i, \\,\n",
      "Nearest to @: ., r, l, °, €, t, ½, /,\n",
      "Nearest to S: s, g, l, t, B, A, v, ',\n",
      "Nearest to U: :, j, Z, -, _, F, @, …,\n",
      "Nearest to K: 1, X, 8, ’, 0, M, D, :,\n",
      "Nearest to Q: T, N, C, \u0014, 9, ., 5, ),\n",
      "Nearest to 4: 5, 8, 1, 6, 0, 9, 3, 7,\n",
      "Average loss at step  92000 :  3.81500869668\n",
      "Average loss at step  94000 :  3.81835488558\n",
      "Average loss at step  96000 :  3.82671459436\n",
      "Average loss at step  98000 :  3.83355979085\n",
      "Average loss at step  100000 :  3.83586820233\n",
      "Nearest to P: M, T, g, D, A, $, C, N,\n",
      "Nearest to p: t, m, c, s, /, \\, w, e,\n",
      "Nearest to 0: 5, 9, 6, 8, 3, 7, 4, 1,\n",
      "Nearest to !: ¯, B, *, °, ¢, H, Ã, ƒ,\n",
      "Nearest to ¯: o, ½, !, Ã, n, x, u, z,\n",
      "Nearest to B: A, 3, S, Y, !, O, E,  ,\n",
      "Nearest to Z: Y, M, U, R, N, X, Æ, -,\n",
      "Nearest to V: R, J, 5, N, F, I, k, B,\n",
      "Nearest to \": p, m, =, â, B, \\, h, ],\n",
      "Nearest to r: n, m, s, d, ‚, b, l, u,\n",
      "Nearest to @: ., °, r, l, €, t, ½, †,\n",
      "Nearest to S: A, 0, O, B, z, :, T, ',\n",
      "Nearest to U: :, Z, j, A, F, N, W, O,\n",
      "Nearest to K: 1, 6, M, X, 9, 3, 0, 8,\n",
      "Nearest to Q: T, N, C, 9, \u0014, Y, 7, Z,\n",
      "Nearest to 4: 5, 1, 7, 6, 8, 9, 0, 3,\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Begin training.\n",
    "from six.moves import xrange\n",
    "\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # We must initialize all variables before we use them.\n",
    "  init.run()\n",
    "  print(\"Initialized\")\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(\n",
    "        batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss /= 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print(\"Average loss at step \", step, \": \", average_loss)\n",
    "      average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in xrange(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "        log_str = \"Nearest to %s:\" % valid_word\n",
    "        for k in xrange(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log_str = \"%s %s,\" % (log_str, close_word)\n",
    "        print(log_str)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 6: Visualize the embeddings.\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "  assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "  plt.figure(figsize=(40, 40))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "\n",
    "  plt.savefig(filename)\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "#import sklearn as sklearn\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "plot_only = vocabulary_size\n",
    "low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "plot_with_labels(low_dim_embs, labels, filename=\"url2vec.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.09693430e-01   1.90923121e-02   8.89237598e-03  -9.12648886e-02\n",
      "    9.89127308e-02  -1.29088402e-01   8.66665877e-03  -1.01243883e-01\n",
      "   -1.31039202e-01  -7.35624060e-02  -1.23370469e-01  -6.05564914e-04\n",
      "    6.03334345e-02   4.33883332e-02   8.53326619e-02   1.58337697e-01\n",
      "   -1.60032928e-01   8.29951316e-02   1.89597365e-02   7.29780421e-02\n",
      "   -4.91956957e-02   1.14263095e-01  -4.86182570e-02  -6.13733381e-02\n",
      "   -2.43748091e-02   1.02300726e-01  -1.24407992e-01   3.16287577e-02\n",
      "   -2.74565797e-02   6.18777424e-02  -1.25284851e-01  -2.01462992e-02\n",
      "    1.59645692e-01   1.21223301e-01   1.08656533e-01  -9.04804692e-02\n",
      "   -1.05627105e-01   1.21519431e-01  -1.31856754e-01  -1.34518459e-01\n",
      "    1.55744374e-01   1.50815904e-01   3.78736258e-02   3.49788852e-02\n",
      "    8.88711885e-02   8.79040435e-02  -1.12346761e-01  -3.66950445e-02\n",
      "    2.41516344e-02   2.33177301e-02  -1.35078531e-04   5.31477109e-03\n",
      "    1.27071425e-01  -1.08586393e-01   2.47703437e-02  -1.40328556e-01\n",
      "    1.29004255e-01   2.32691057e-02   7.04981238e-02   1.98147260e-02\n",
      "   -9.02393460e-03   5.42227812e-02   5.59383556e-02   2.52181804e-03\n",
      "   -1.56339016e-02  -1.23796828e-01   6.03650026e-02   6.15955368e-02\n",
      "   -9.90277380e-02   4.70469184e-02  -9.06546712e-02  -5.36697954e-02\n",
      "   -6.89292848e-02  -1.12400003e-01  -8.70861784e-02   1.01187624e-01\n",
      "    1.05320141e-01  -5.46062663e-02  -1.24767140e-01   1.02210790e-01\n",
      "   -7.46736676e-02  -6.20144643e-02  -2.70835776e-02   6.79749772e-02\n",
      "   -3.59547921e-02  -2.21329276e-02   1.08199313e-01   1.41553059e-01\n",
      "   -1.02312304e-01  -1.22783765e-01   9.19621885e-02  -1.46970758e-02\n",
      "    1.38285279e-01  -5.40212402e-03   7.09781945e-02   8.76052901e-02\n",
      "    3.29851769e-02   1.42864302e-01  -7.48558044e-02  -4.52511124e-02\n",
      "    1.25784278e-01  -1.62529629e-02   1.32769912e-01   2.03075483e-02\n",
      "   -7.17593208e-02   8.74957964e-02  -1.28881156e-01   4.41913716e-02\n",
      "    3.68401743e-02  -1.39882371e-01  -4.31148931e-02  -1.39898598e-01\n",
      "    7.71517456e-02   5.90422265e-02   2.28668042e-02  -5.92389181e-02\n",
      "   -2.81435512e-02   5.54076619e-02  -6.51655942e-02   2.39483323e-02\n",
      "   -1.96314920e-02   1.04116946e-01   1.02623418e-01   1.00152921e-02\n",
      "   -8.95571783e-02   1.29427373e-01  -1.14789642e-01   1.58207357e-01]\n",
      " [ -2.93011833e-02   7.18268566e-03  -1.79902986e-02  -1.71244668e-03\n",
      "   -6.48273826e-02  -9.62403864e-02   2.51010954e-02   3.31726708e-02\n",
      "    1.15074016e-01   5.01061566e-02   1.32864133e-01  -1.41831622e-01\n",
      "    9.15923864e-02  -2.19400842e-02  -2.12417692e-02   5.85962348e-02\n",
      "   -1.84150547e-01  -4.12506834e-02   1.19921923e-01  -1.53937051e-02\n",
      "    7.14079663e-02  -4.57507782e-02   1.13381753e-02  -6.09128661e-02\n",
      "    1.37138575e-01   4.65954728e-02   1.21661454e-01  -1.56065330e-01\n",
      "    5.87828644e-02  -3.13616395e-02   1.16706463e-02  -1.40693113e-02\n",
      "    8.51826370e-02   2.47856155e-02  -5.25728613e-02  -2.01314956e-01\n",
      "    8.48838761e-02   2.98886430e-02  -5.52722719e-03  -8.61883014e-02\n",
      "    2.08933931e-02   2.77571287e-02  -7.92303868e-03   1.28926858e-01\n",
      "   -1.57761514e-01   1.62560999e-01  -1.20705783e-01  -3.68605666e-02\n",
      "   -3.54768746e-02  -1.55674040e-01  -1.85618460e-01   2.13987142e-01\n",
      "    2.57051826e-01  -9.87927392e-02  -1.52219161e-01  -9.37401578e-02\n",
      "    5.58180325e-02  -6.37547225e-02  -5.54588921e-02   1.23106008e-02\n",
      "    1.59984156e-01   4.92082536e-03   1.03494786e-01  -4.90290299e-02\n",
      "    2.32484043e-02   2.27703508e-02  -1.73186883e-01  -1.81648731e-01\n",
      "   -6.78531080e-02  -7.69283040e-04  -3.03490404e-02  -6.10229885e-03\n",
      "    4.27784882e-02   1.47411764e-01   1.51401863e-01  -4.66179997e-02\n",
      "    4.77604046e-02   5.93999512e-02   7.84802716e-03  -6.34171292e-02\n",
      "   -1.10787474e-01   5.31441681e-02   1.69350818e-01  -4.10970487e-03\n",
      "    1.21325925e-01   6.28498718e-02  -1.40520548e-02   1.11463971e-01\n",
      "    3.26541737e-02  -5.32690994e-02   2.89975647e-02   3.87767330e-02\n",
      "   -1.73605257e-03   1.00705633e-02  -5.41924462e-02  -6.10510632e-02\n",
      "    5.13023511e-02  -2.19980106e-02  -3.70443128e-02  -1.47756383e-01\n",
      "    5.18580601e-02   4.50806180e-03  -6.11024797e-02   1.04205571e-01\n",
      "   -1.89112052e-01   5.45994425e-03   9.71896516e-04  -6.16486110e-02\n",
      "   -4.88627553e-02  -4.58116941e-02   5.23030385e-02  -1.11267194e-01\n",
      "    1.86330918e-02  -1.24869443e-01   5.88484183e-02  -1.01572581e-01\n",
      "    1.13254366e-02  -6.39303178e-02  -4.33024243e-02  -5.18856533e-02\n",
      "   -3.55167571e-03   5.18850982e-02  -1.13475762e-01  -3.27174366e-02\n",
      "   -3.53618083e-03  -2.24089194e-02   1.55057535e-02   3.99930812e-02]\n",
      " [ -7.35416785e-02  -1.02089427e-01   7.20832124e-02   1.12917610e-02\n",
      "   -6.92627728e-02   1.67179704e-02   7.09020579e-03  -1.04253918e-01\n",
      "   -5.96155785e-02   1.09693939e-02  -1.16989696e-02  -7.81906769e-02\n",
      "   -6.93199113e-02   3.27514261e-02   1.38647944e-01   1.48652503e-02\n",
      "    4.09529060e-02  -1.42128304e-01   1.21524200e-01   9.63955671e-02\n",
      "   -4.37681228e-02  -6.72079027e-02   1.02531001e-01   5.98302521e-02\n",
      "    1.59723356e-01   1.19553832e-02   1.29402712e-01   3.91131826e-02\n",
      "    1.81245077e-02   1.69529989e-01   1.71130877e-02  -1.74346700e-01\n",
      "    2.23352686e-01  -1.82344526e-01   4.68986258e-02   1.13640122e-01\n",
      "    2.07250398e-02   2.64456533e-02  -4.89804149e-02   2.17703786e-02\n",
      "   -3.08813062e-04   2.58131921e-02  -2.07288703e-03   3.26102264e-02\n",
      "   -2.24810056e-02   1.95009246e-01  -5.28240837e-02  -6.82758540e-02\n",
      "    7.66826794e-02   1.04199566e-01  -6.31998852e-02   1.21531166e-01\n",
      "    1.55122485e-02   5.52879795e-02  -6.59391508e-02  -8.98016468e-02\n",
      "   -3.13965455e-02  -7.08508939e-02   4.64420430e-02   7.41088763e-02\n",
      "    1.54410275e-02  -1.52884442e-02   9.31676328e-02   1.51952161e-02\n",
      "    9.61262807e-02  -1.32784266e-02  -1.07112274e-01   3.16323228e-02\n",
      "   -6.02920540e-02  -1.61532238e-02  -1.05056174e-01   2.21132245e-02\n",
      "   -8.80858526e-02  -4.22632881e-02   6.88902512e-02  -9.08449641e-04\n",
      "   -9.07298848e-02   5.98664023e-02  -6.55282661e-02  -1.11773983e-01\n",
      "    1.14818893e-01   4.14026007e-02   1.53654039e-01  -8.72123390e-02\n",
      "    4.40556407e-02   1.65347591e-01  -5.01533709e-02   1.36835715e-02\n",
      "    1.21828184e-01   2.97903158e-02  -1.43705547e-01  -8.62629935e-02\n",
      "   -4.52277884e-02  -1.26326099e-01  -6.55407161e-02   8.74878392e-02\n",
      "   -9.40819681e-02  -1.36263728e-01   4.48831655e-02  -1.23313673e-01\n",
      "    1.29074663e-01   1.54886186e-01   5.33331074e-02   3.19584794e-02\n",
      "    4.83899266e-02  -2.33355463e-02   2.13565528e-02   2.01701999e-01\n",
      "   -5.61932325e-02  -1.36267766e-01   5.55963181e-02   3.43256742e-02\n",
      "    1.61961034e-01   3.69855464e-02  -2.70974524e-02  -5.26927523e-02\n",
      "   -5.01860119e-02  -5.86958049e-05   4.29257043e-02  -2.12517783e-01\n",
      "   -5.33701945e-03  -3.51332724e-02  -1.13048635e-01  -3.69781069e-02\n",
      "    1.56545758e-01   1.02003597e-01  -4.25736643e-02   9.59432349e-02]\n",
      " [ -1.07873455e-02  -9.98418331e-02  -3.53472382e-02  -4.34535220e-02\n",
      "   -4.38177660e-02   3.58325914e-02   4.95617166e-02  -8.84943977e-02\n",
      "    6.50960654e-02  -4.34392355e-02   5.25494590e-02  -1.52896464e-01\n",
      "    2.07478330e-02  -8.48384351e-02   1.42562538e-01  -1.19271483e-02\n",
      "    9.47984159e-02  -1.19986124e-01   1.99526269e-02   1.53524116e-01\n",
      "    7.17622265e-02  -1.66965779e-02   5.81958890e-02  -4.66233604e-02\n",
      "    3.87310944e-02  -8.01706240e-02   1.15561910e-01   1.21695012e-01\n",
      "    2.84895264e-02   9.36578438e-02   5.70941269e-02   5.71120754e-02\n",
      "   -7.02194422e-02   4.17913496e-03   1.01443186e-01   2.28773598e-02\n",
      "    2.24519037e-02  -1.20490678e-02  -8.06954354e-02   1.89357042e-01\n",
      "    3.31631929e-01  -4.21693996e-02   6.72613308e-02  -9.00793541e-03\n",
      "    1.58787780e-02   7.03420863e-02  -7.64132887e-02  -2.14570925e-01\n",
      "   -1.67219918e-02   9.91084650e-02  -3.99955362e-02  -3.02310530e-02\n",
      "    2.66659558e-02  -1.02003753e-01  -6.87309057e-02   9.42608416e-02\n",
      "    8.96974877e-02   3.70720550e-02  -1.20774761e-01   6.02224767e-02\n",
      "    1.19835697e-02  -8.65725055e-03   4.97364812e-02   1.21982582e-02\n",
      "    8.27956125e-02  -5.35179637e-02  -5.23143373e-02   1.58777405e-02\n",
      "   -1.80388764e-01  -6.29985258e-02  -8.44311491e-02   4.80577685e-02\n",
      "    1.50950551e-01   9.81247276e-02  -7.63808237e-03   1.42384991e-01\n",
      "   -3.97265479e-02  -8.23214352e-02  -1.00030508e-02  -1.86826140e-02\n",
      "    5.96324448e-03  -3.14504951e-02   1.95659280e-01   4.89708297e-02\n",
      "    5.97840883e-02  -2.71237711e-03   2.45463010e-02   2.25324780e-04\n",
      "   -4.99227159e-02  -6.67847767e-02   3.87403220e-02  -3.51620801e-02\n",
      "   -1.23300232e-01   6.48202375e-02  -2.33535841e-02   1.21668637e-01\n",
      "    5.55854328e-02   1.72533970e-02  -7.41965622e-02  -1.50278211e-01\n",
      "    1.10335134e-01   5.34631647e-02  -4.26927768e-03   2.04096921e-02\n",
      "   -1.97772473e-01  -1.02513954e-01   6.85055926e-02   1.11272261e-01\n",
      "   -1.19979031e-01   5.49684390e-02  -1.25897641e-03   5.32048987e-03\n",
      "    5.53342775e-02   6.73406422e-02  -8.20969976e-03   2.36810669e-02\n",
      "   -7.97326341e-02   3.31377760e-02  -1.51024789e-01  -1.77389771e-01\n",
      "   -3.95500846e-02  -9.21642929e-02  -3.99653725e-02  -1.36804640e-01\n",
      "   -1.15628697e-01  -8.68012104e-03   8.97401124e-02   1.97239429e-01]\n",
      " [  4.85572778e-02   4.80317511e-02   1.88706174e-01   1.77793279e-02\n",
      "    7.82240853e-02  -1.90273784e-02  -8.05054009e-02  -2.04143953e-02\n",
      "    6.14355803e-02   6.04094304e-02  -7.26489862e-03  -8.11617076e-02\n",
      "    1.54246777e-01   8.07192549e-03   5.71379364e-02   5.66228963e-02\n",
      "    4.77523170e-02  -4.38205153e-02   1.06412143e-01  -1.41488817e-02\n",
      "    6.06865697e-02  -6.84030801e-02  -5.02854958e-02  -6.92650899e-02\n",
      "    9.39921960e-02  -1.42853647e-01   2.08442777e-01  -1.92891397e-02\n",
      "    9.13108736e-02   1.22920042e-02   7.30099855e-03   2.80058081e-03\n",
      "   -1.81962755e-02   7.91098177e-02   9.04091671e-02  -3.43279764e-02\n",
      "    7.37316310e-02   1.43856049e-01  -3.49933542e-02  -4.75771986e-02\n",
      "    2.89334595e-01  -1.75497532e-02   3.92835028e-02   1.61151551e-02\n",
      "   -2.10914195e-01   5.25837690e-02  -1.06082827e-01  -1.11072160e-01\n",
      "   -2.30916347e-02   3.08202710e-02  -1.71562940e-01   5.85883632e-02\n",
      "    1.15169160e-01   7.88422115e-03  -5.84002398e-02  -1.42053813e-01\n",
      "    1.48699343e-01  -9.49738696e-02  -9.76406708e-02   1.00081496e-01\n",
      "    4.46099080e-02  -5.20450957e-02  -2.18303706e-02  -1.20586939e-01\n",
      "   -2.73846500e-02   3.87010071e-03  -5.90525232e-02  -1.25508696e-01\n",
      "   -5.14429947e-03  -6.52397051e-02  -6.27318993e-02  -1.00453332e-01\n",
      "    7.97229111e-02   5.15891016e-02   9.89499018e-02   1.20454421e-02\n",
      "    6.54208586e-02   1.36204753e-02  -3.25297005e-02  -1.47650942e-01\n",
      "    8.97565261e-02   1.44709036e-01   1.57568187e-01   7.81859383e-02\n",
      "    9.67881922e-03   1.01420857e-01  -1.25326008e-01  -5.81317768e-02\n",
      "   -3.55384573e-02   4.74214107e-02  -2.17696652e-02   1.52010135e-02\n",
      "   -8.49825069e-02   4.96807992e-02  -3.35080251e-02  -4.78861891e-02\n",
      "    1.08230166e-01   7.65556190e-03   2.26567965e-02  -2.49033347e-01\n",
      "    1.25357723e-02   2.35728305e-02   1.00959830e-01  -1.51583031e-02\n",
      "   -2.40179431e-02   2.87222657e-02   3.52045149e-02   1.07102938e-01\n",
      "   -3.93065158e-03  -1.81475013e-01  -8.11354890e-02  -1.05179742e-01\n",
      "    9.60357934e-02  -1.66656002e-01   5.70601597e-02   6.69808611e-02\n",
      "   -4.46698889e-02  -4.62543638e-03   7.45555684e-02  -1.34053037e-01\n",
      "    7.93218911e-02  -1.27401669e-02   1.53132956e-02  -1.44517869e-01\n",
      "   -1.51226642e-02  -5.76658882e-02  -3.67436782e-02  -2.23172475e-02]] ['UNK', 't', '/', 'e', 's']\n"
     ]
    }
   ],
   "source": [
    "print(final_embeddings[:5], labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
